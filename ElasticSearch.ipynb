{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "import numpy as np\n",
    "from bert_serving.client import BertClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Config\n",
    "INDEX_NAME = 'posts'\n",
    "INDEX_FILE = os.path.join(os.getcwd(), \"data\\index.json\")\n",
    "DATA_FILE = os.path.join(os.getcwd(), \"data\\posts.json\")\n",
    "BATCH_SIZE = 1000\n",
    "SEARCH_SIZE = 5\n",
    "\n",
    "#Client Object for Elastic Search\n",
    "client = Elasticsearch(timeout=100)\n",
    "bc = BertClient(ip='137.117.84.80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding Data / Indexing Data\n",
    "\n",
    "def indexData():\n",
    "    print('Creating Index .....')\n",
    "    client.indices.delete(index = INDEX_NAME ,ignore = [404])\n",
    "    \n",
    "    with open(INDEX_FILE) as index_file:\n",
    "        source = index_file.read().strip()\n",
    "        client.indices.create(index=INDEX_NAME,body=source)\n",
    "        \n",
    "    docs = []\n",
    "    count= 0\n",
    "    with open(DATA_FILE) as data_file:\n",
    "        for line in data_file:\n",
    "            line = line.strip()\n",
    "\n",
    "            doc = json.loads(line)\n",
    "            if doc['type'] != 'question':\n",
    "                continue\n",
    "\n",
    "            docs.append(doc)\n",
    "            count+=1\n",
    "\n",
    "            if count % BATCH_SIZE == 0 :\n",
    "                index_batch(docs)\n",
    "                docs = []\n",
    "                print(\"Indexed {} Documents\".format(count))\n",
    "        \n",
    "        if docs:\n",
    "            index_batch(docs)\n",
    "            print(\"Indexed {} Documents\".format(count))\n",
    "            \n",
    "    client.indices.refresh(index=INDEX_NAME)\n",
    "    print(\"Done Indexing\")\n",
    "                \n",
    "        \n",
    "def index_batch(docs):\n",
    "    titles = [doc['title'] for doc in docs]\n",
    "    title_vectors = embed_text(titles)\n",
    "    requests = []\n",
    "    for i,doc in enumerate(docs):\n",
    "        request = doc\n",
    "        request[\"_op_type\"] = 'index'\n",
    "        request[\"_index\"] = INDEX_NAME\n",
    "        request['title_vector'] = title_vectors[i]\n",
    "        requests.append(request)\n",
    "    bulk(client,requests)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding Text \n",
    "def embed_text(text):\n",
    "    vectors = bc.encode(text)\n",
    "    return [vector.tolist() for vector in vectors]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Index .....\n",
      "Indexed 1000 Documents\n",
      "Indexed 2000 Documents\n",
      "Indexed 3000 Documents\n",
      "Indexed 4000 Documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\An\\lib\\site-packages\\bert_serving\\client\\__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=25\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
      "here is what you can do:\n",
      "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
      "- or, start a new server with a larger \"max_seq_len\"\n",
      "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 5000 Documents\n",
      "Indexed 6000 Documents\n",
      "Indexed 7000 Documents\n",
      "Indexed 8000 Documents\n",
      "Indexed 9000 Documents\n",
      "Indexed 10000 Documents\n",
      "Indexed 11000 Documents\n",
      "Indexed 12000 Documents\n",
      "Indexed 13000 Documents\n",
      "Indexed 14000 Documents\n",
      "Indexed 15000 Documents\n",
      "Indexed 16000 Documents\n",
      "Indexed 17000 Documents\n",
      "Indexed 18000 Documents\n",
      "Indexed 18848 Documents\n",
      "Done Indexing\n"
     ]
    }
   ],
   "source": [
    "indexData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "def query():\n",
    "    while True:\n",
    "        handle_query()\n",
    "        \n",
    "## Searching               \n",
    "def handle_query():\n",
    "    query = input(\"Enter Query - \")\n",
    "    query_vector = embed_text([query])[0]\n",
    "    \n",
    "    script_query = {\n",
    "        \"script_score\":{\n",
    "            \"query\":{\"match_all\":{}},\n",
    "            \"script\": {\n",
    "                \"source\":\"cosineSimilarity(params.query_vector, doc['title_vector']) + 1.0\",\n",
    "                \"params\": {\"query_vector\": query_vector}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = client.search(index=INDEX_NAME,body={\n",
    "            \"size\": SEARCH_SIZE,\n",
    "            \"query\": script_query,\n",
    "            \"_source\": {\"includes\": [\"title\", \"body\"]}\n",
    "        }\n",
    "    )\n",
    "    print(\"{} total hits.\".format(response[\"hits\"][\"total\"][\"value\"]))\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        print(\"id: {}, score: {}\".format(hit[\"_id\"], hit[\"_score\"]))\n",
    "        print(hit[\"_source\"])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query - onthology\n",
      "10000 total hits.\n",
      "id: 22lcsG0BEtnmCY9HQ-Os, score: 1.8241231\n",
      "{'title': 'Genealogy Tree Control', 'body': \"I've been tasked (by my wife) with creating a program to allow her to track the family trees on both sides of our family. Does anyone know of a cost effective (free) control to represent this type of information. What I'm looking for is a modified org-chart type chart/tree. The modification is that any node should have 2 parent nodes (E.G. a child should have a Mother/Father). The solution I've came up with so far is to have 2 trees, an ancestor tree and a descendants tree, with the individual being inspected as the root node for each tree. It works, but is sort of clunky. I'm working primarily in c# WinForms, so .Net type controls or source code is preferrable. \"}\n",
      "\n",
      "id: xGlfsG0BEtnmCY9HfP1K, score: 1.8233192\n",
      "{'title': 'Asynchronous APIs', 'body': \"When trying to implement an asynchronous API calls / Non-blocking calls, I know a little in a All Plain-C application I have, I read a about APM (Asynchronous Programming Model) by 'Delegates'. Basically what I want to do is call one API f1() to do a functionality(which takes long time 8-10 seconds), So I call that API f1(), forget about it, and continue doing some other work, e.g. I/O for to fetch data for next call of the f1() or some functionality not dependent on result of f1(). If any one has used that APM model of programming, I am looking at some concise explanation for implementing non-blocking calls. Is there any other way of implementing asynchronous APIs , any other library/framework which might help in this? \"}\n",
      "\n",
      "id: l2ldsG0BEtnmCY9H2-88, score: 1.8213824\n",
      "{'title': 'How to turn off sounds in TortoiseSVN?', 'body': 'I do not want TortoiseSVN to alert me with sounds - e.g. when it fails to update. How do I turn off sounds in TortoiseSVN? '}\n",
      "\n",
      "id: imphsG0BEtnmCY9HzAqc, score: 1.8177309\n",
      "{'title': 'Software Design Description Practise', 'body': \"How many people actually write an SDD document before writing a single line of code? How do you handle large CSCI's? What standard do you use for SDD content? What tailoring have you done? \"}\n",
      "\n",
      "id: e2pgsG0BEtnmCY9H-wZA, score: 1.817603\n",
      "{'title': 'Charting in web-based applications', 'body': 'What are the various charting tools that are available for displaying charts on a web page using ASP.NET? I know about commercial tools such as Dundas and Infragistics. I could have \"googled\" this but I want to know the various tools that SO participants have used? Any free charting tools that are available are also welcome to be mentioned. '}\n",
      "\n",
      "Enter Query - how to update sql table\n",
      "10000 total hits.\n",
      "id: wmlesG0BEtnmCY9Huvic, score: 1.9741544\n",
      "{'title': 'How to import a DBF file in SQL Server', 'body': 'How can you import a foxpro DBF file in SQL Server? '}\n",
      "\n",
      "id: NmldsG0BEtnmCY9H0-1c, score: 1.9678193\n",
      "{'title': 'How to select an SQL database?', 'body': \"We're living in a golden age of databases, with numerous high quality commercial and free databases. This is great, but the downside is there's not a simple obvious choice for someone who needs a database for his next project. What are the constraints/criteria you use for selecting a database? How well do the various databases you've used meet those constraints/criteria? What special features do the databases have? Which databases do you feel comfortable recommending to others? etc... \"}\n",
      "\n",
      "id: tmplsG0BEtnmCY9H4x35, score: 1.9661198\n",
      "{'title': 'How to check if a column exists in SQL Server table', 'body': \"I need to add a specific column if it is does not exist. I have something like this, but it always returns false: IF EXISTS(SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'myTableName' AND COLUMN_NAME = 'myColumnName') How can I check if a column exists in a table of SQL Server database? \"}\n",
      "\n",
      "id: 0WlfsG0BEtnmCY9HfP1K, score: 1.9657485\n",
      "{'title': 'How to update large XML file', 'body': 'Rather than rewriting the entire contents of an xml file when a single element is updated, is there a better alternative to updating the file? '}\n",
      "\n",
      "id: WmpnsG0BEtnmCY9HJyMb, score: 1.9644285\n",
      "{'title': 'How to format Oracle SQL text-only select output', 'body': \"I am using Oracle SQL (in SQLDeveloper, so I don't have access to SQLPLUS commands such as COLUMN) to execute a query that looks something like this: select assigner_staff_id as staff_id, active_flag, assign_date, complete_date, mod_date from work where assigner_staff_id = '2096'; The results it give me look something like this: STAFF_ID ACTIVE_FLAG ASSIGN_DATE COMPLETE_DATE MOD_DATE ---------------------- ----------- ------------------------- ------------------------- ------------------------- 2096 F 25-SEP-08 27-SEP-08 27-SEP-08 02.27.30.642959000 PM 2096 F 25-SEP-08 25-SEP-08 25-SEP-08 01.41.02.517321000 AM 2 rows selected This can very easily produce a very wide and unwieldy textual report when I'm trying to paste the results as a nicely formatted quick-n-dirty text block into an e-mail or problem report, etc. What's the best way to get rid of all tha extra white space in the output columns when I'm using just plain-vanilla Oracle SQL? So far all my web searches haven't turned up much, as all the web search results are showing me how to do it using formatting commands like COLUMN in SQLPLUS (which I don't have). \"}\n",
      "\n",
      "Enter Query - my sql table replace\n",
      "10000 total hits.\n",
      "id: 7mlcsG0BEtnmCY9HOeJg, score: 1.9357256\n",
      "{'title': 'SQL query for a database scheme', 'body': 'In SQL Server how do you query a database to bring back all the tables that have a field of a specific name? '}\n",
      "\n",
      "id: 6GlesG0BEtnmCY9HsfVY, score: 1.9344633\n",
      "{'title': 'Setting Up MySQL Triggers', 'body': \"I've been hearing about triggers, and I have a few questions. What are triggers? How do I set them up? Are there any precautions, aside from typical SQL stuff, that should be taken? \"}\n",
      "\n",
      "id: QGpnsG0BEtnmCY9HJyMb, score: 1.934096\n",
      "{'title': 'Wrap a SQL Reporting Matrix', 'body': 'I have a matrix in SQL reporting and I would like it to print on an A4 page. If the matrix has less than 4 columns then it fits but for more than 4 columns I would like the matrix to wrap and show only 4 columns per page. Is this possible? I am using SQL Reporting 2005 in localmode. '}\n",
      "\n",
      "id: _WposG0BEtnmCY9HQSmF, score: 1.9335697\n",
      "{'title': 'How to import a SQL Server .bak file into MySQL?', 'body': 'The title is self explanatory. Is there a way of directly doing such kind of importing? '}\n",
      "\n",
      "id: NmldsG0BEtnmCY9H0-1c, score: 1.9335239\n",
      "{'title': 'How to select an SQL database?', 'body': \"We're living in a golden age of databases, with numerous high quality commercial and free databases. This is great, but the downside is there's not a simple obvious choice for someone who needs a database for his next project. What are the constraints/criteria you use for selecting a database? How well do the various databases you've used meet those constraints/criteria? What special features do the databases have? Which databases do you feel comfortable recommending to others? etc... \"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bert-serving-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flask API \n",
    "\n",
    "\n",
    "from flask import Flask, request, render_template\n",
    "import flask\n",
    "from flask_cors import CORS, cross_origin\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app, support_credentials=True)\n",
    "\n",
    "@app.route('/')\n",
    "def my_form():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/', methods=['POST'])\n",
    "def my_form_post():\n",
    "    text = request.form['text']\n",
    "    processed_text = text.upper()\n",
    "    return processed_text\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    app.run(host=None,port=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
